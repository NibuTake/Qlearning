{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "0 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 1\n",
      "100 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 0\n",
      "200 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 0\n",
      "300 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 0\n",
      "400 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 1\n",
      "500 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 1\n",
      "600 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 0\n",
      "700 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 1\n",
      "800 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 1\n",
      "900 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 0\n",
      "1000 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 0\n",
      "1100 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 1\n",
      "1200 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 1\n",
      "1300 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 0\n",
      "1400 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 2\n",
      "1500 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 2\n",
      "1600 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 0\n",
      "1700 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 1\n",
      "1800 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 2\n",
      "1900 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 2\n",
      "2000 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 0\n",
      "2100 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 2\n",
      "2200 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 1\n",
      "2300 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 0\n",
      "2400 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 0\n",
      "2500 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 1\n",
      "2600 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 0\n",
      "2700 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 2\n",
      "2800 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 2\n",
      "2900 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 1\n"
     ]
    }
   ],
   "source": [
    "# Import packages.\n",
    "import gym\n",
    "from gym import envs\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define environment.\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "rewards = []\n",
    "observation = env.reset()\n",
    "\n",
    "num_episodes = 3000\n",
    "\n",
    "gamma = 0.99\n",
    "batch_size = 50\n",
    "\n",
    "decision_net = QNetwork(hidden_size=8, learning_rate=0.001)\n",
    "target_net = QNetwork(hidden_size=8, learning_rate=0.001)\n",
    "\n",
    "memory = Memory(max_size=30000)\n",
    "actor = Actor()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    state = state.reshape(1,2)\n",
    "    episode_reward = 0\n",
    "    \n",
    "    target_net.model.set_weights(decision_net.model.get_weights())\n",
    "        \n",
    "    for t, _ in enumerate(range(200)):\n",
    "        action = actor.get_action(state, episode, decision_net)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = next_state.reshape(1,2)\n",
    "        \n",
    "        if done:\n",
    "            if t+2 < 200:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "        episode_reward += reward\n",
    "        \n",
    "        memory.add((state, action, reward, next_state))\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            if episode%100 == 0:\n",
    "                print(str(episode) + \" : Episode finished after {} timesteps\".format(t+2))\n",
    "                print('reward', reward)\n",
    "                print('action', actor.get_action(state, episode, decision_net))\n",
    "\n",
    "            # print(episode_reward)\n",
    "            break\n",
    "        \n",
    "        # Qネットワークの重みを学習・更新する replay\n",
    "        if (memory.len() > batch_size) & (t%50 == 0):\n",
    "            decision_net.replay(memory, batch_size, gamma, target_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "state, action, reward, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huberloss(y_true, y_pred):\n",
    "    err = y_true - y_pred\n",
    "    cond = K.abs(err) < 1.0\n",
    "    L2 = 0.5 * K.square(err)\n",
    "    L1 = (K.abs(err) - 0.5)\n",
    "    loss = tf.where(cond, L2, L1)  # Keras does not cover where function in tensorflow :-(\n",
    "    return K.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9967901922995225"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_target(gamma, q_value.predict(np.array([1,2]).reshape(1,2)).max(), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_net = network()\n",
    "target_net =network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, learning_rate=0.01, state_size=2, action_size=3, hidden_size=10):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(hidden_size, activation='relu', input_dim=state_size,\n",
    "                             kernel_initializer=initializers.TruncatedNormal(stddev=0.01)))\n",
    "        self.model.add(Dense(hidden_size, activation='relu',\n",
    "                             kernel_initializer=initializers.TruncatedNormal(stddev=0.01)))\n",
    "        self.model.add(Dense(action_size, activation='sigmoid',\n",
    "                             kernel_initializer=initializers.TruncatedNormal(stddev=0.01)))\n",
    "        self.optimizer = Adam(lr=learning_rate)  # 誤差を減らす学習方法はAdam\n",
    "        self.model.compile(loss=huberloss, optimizer=self.optimizer)\n",
    "        \n",
    "    def replay(self, memory, batch_size, gamma, target_net):\n",
    "        inputs = np.zeros((batch_size, 2))\n",
    "        Targets = np.zeros((batch_size, 3))\n",
    "        mini_batch = memory.sample(batch_size)\n",
    "        \n",
    "        for i, (state_b, action_b, reward_b, next_state_b) in enumerate(mini_batch):\n",
    "            inputs[i:i+1] = state_b\n",
    "            target = reward_b\n",
    "            \n",
    "            next_action_for_estimate = decision_net.model.predict(next_state_b).argmax()\n",
    "            q_value_for_estimate = target_net.model.predict(next_state_b)[0][next_action_for_estimate]\n",
    "            target = reward_b + gamma * q_value_for_estimate\n",
    "            \n",
    "            Targets[i][action_b] = target\n",
    "            \n",
    "        self.model.fit(inputs, Targets, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def get_action(self, state, episode, decision_net):   # [C]ｔ＋１での行動を返す\n",
    "        # 徐々に最適行動のみをとる、ε-greedy法\n",
    "        epsilon = 0.001 + 0.9 / (1.0+episode)\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            q_values_for_action = decision_net.model.predict(state)[0]\n",
    "            action = np.argmax(q_values_for_action)  # 最大の報酬を返す行動を選択する\n",
    "\n",
    "        else:\n",
    "            action = np.random.choice([0, 1])  # ランダムに行動する\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.79167402e-01 -2.23429532e-03]\n",
      " [-3.77280371e-01  1.09030105e-03]\n",
      " [-4.15241087e-01  3.05925841e-03]\n",
      " [-4.48024754e-01 -6.99231946e-04]\n",
      " [-4.09522138e-01 -2.52015430e-03]\n",
      " [-3.37958122e-01  1.65150625e-04]\n",
      " [-3.81451355e-01 -2.28395226e-03]\n",
      " [-3.70677484e-01  1.10844018e-04]\n",
      " [-3.88444139e-01 -2.33847550e-03]\n",
      " [-3.75294196e-01  9.58358498e-04]\n",
      " [-4.41297322e-01 -1.75807573e-03]\n",
      " [-4.33743462e-01  1.44951371e-04]\n",
      " [-4.15179901e-01  4.49835656e-03]\n",
      " [-3.66442667e-01 -4.49703695e-03]\n",
      " [-3.71327620e-01  4.24819190e-04]\n",
      " [-3.55116305e-01 -1.55998483e-04]\n",
      " [-4.25759342e-01 -2.36673010e-03]\n",
      " [-4.20766160e-01 -2.86738320e-03]\n",
      " [-4.44636537e-01 -1.97013075e-03]\n",
      " [-3.88091843e-01  2.57874854e-03]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [-1.,  0.,  0.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 20\n",
    "inputs = np.zeros((batch_size, 2))\n",
    "Targets = np.zeros((batch_size, 3))\n",
    "mini_batch = memory.sample(batch_size)\n",
    "for i, (state_b, action_b, reward_b, next_state_b) in enumerate(mini_batch):\n",
    "    inputs[i:i+1] = state_b\n",
    "    target = reward_b\n",
    "    next_action_for_estimate = decision_net.predict(next_state_b.reshape(1,2)).argmax()\n",
    "    q_value_for_estimate = target_net.predict(next_state_b.reshape(1,2))[0][next_action_for_estimate]\n",
    "    target = reward_b + gamma * q_value_for_estimate\n",
    "    Targets[i][action_b] = target\n",
    "    \n",
    "print(inputs)\n",
    "Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 5],\n",
       "       [1, 2],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.array([1,5]), np.array([1,2]), np.array([1,2])]).reshape(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(max_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.add((observation, action, reward, next_observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.35605739, -0.00057459]), 2, -1.0, array([-0.35683629, -0.0007789 ]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.sample(20)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
