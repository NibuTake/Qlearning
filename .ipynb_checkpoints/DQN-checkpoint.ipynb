{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "iteration 200\n",
      "0 : Episode finished after 200 timesteps\n",
      "reward -1\n",
      "action 1\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n",
      "iteration 200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-e42444b67d3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Qネットワークの重みを学習・更新する replay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mdecision_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-85-592808e1e0bf>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self, memory, batch_size, gamma, target_net)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mnext_action_for_estimate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecision_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mq_value_for_estimate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_action_for_estimate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward_b\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mq_value_for_estimate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1170\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1172\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    295\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import packages.\n",
    "import gym\n",
    "from gym import envs\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define environment.\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "rewards = []\n",
    "observation = env.reset()\n",
    "\n",
    "num_episodes = 3000\n",
    "\n",
    "gamma = 0.99\n",
    "batch_size = 100\n",
    "\n",
    "decision_net = QNetwork(hidden_size=8, learning_rate=0.00001)\n",
    "target_net = QNetwork(hidden_size=8, learning_rate=0.00001)\n",
    "\n",
    "memory = Memory(max_size=5000)\n",
    "actor = Actor()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    state = state.reshape(1,2)\n",
    "    episode_reward = 0\n",
    "    \n",
    "    target_net.model.set_weights(decision_net.model.get_weights())\n",
    "        \n",
    "    for t, _ in enumerate(range(200)):\n",
    "        action = actor.get_action(state, episode, decision_net)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = next_state.reshape(1,2)\n",
    "        \n",
    "        if done:\n",
    "            if t+2 < 200:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "        episode_reward += reward\n",
    "        \n",
    "        memory.add((state, action, reward, next_state))\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            if episode%100 == 0:\n",
    "                print(str(episode) + \" : Episode finished after {} timesteps\".format(t+2))\n",
    "                print('reward', reward)\n",
    "                print('action', actor.get_action(state, episode, decision_net))\n",
    "\n",
    "            # print(episode_reward)\n",
    "            break\n",
    "        \n",
    "        # Qネットワークの重みを学習・更新する replay\n",
    "        if (memory.len() > batch_size) & (t%50 == 0):\n",
    "            decision_net.replay(memory, batch_size, gamma, target_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "state, action, reward, next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huberloss(y_true, y_pred):\n",
    "    err = y_true - y_pred\n",
    "    cond = K.abs(err) < 1.0\n",
    "    L2 = 0.5 * K.square(err)\n",
    "    L1 = (K.abs(err) - 0.5)\n",
    "    loss = tf.where(cond, L2, L1)  # Keras does not cover where function in tensorflow :-(\n",
    "    return K.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9967901922995225"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_target(gamma, q_value.predict(np.array([1,2]).reshape(1,2)).max(), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_net = network()\n",
    "target_net =network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, learning_rate=0.01, state_size=2, action_size=3, hidden_size=10):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(hidden_size, activation='relu', input_dim=state_size,\n",
    "                             kernel_initializer=initializers.TruncatedNormal(stddev=0.1)))\n",
    "        self.model.add(Dense(hidden_size, activation='relu',\n",
    "                             kernel_initializer=initializers.TruncatedNormal(stddev=0.1)))\n",
    "        self.model.add(Dense(action_size, activation='linear',\n",
    "                             kernel_initializer=initializers.TruncatedNormal(stddev=0.1)))\n",
    "        self.optimizer = Adam(lr=learning_rate)  # 誤差を減らす学習方法はAdam\n",
    "        self.model.compile(loss=huberloss, optimizer=self.optimizer)\n",
    "        \n",
    "    def replay(self, memory, batch_size, gamma, target_net):\n",
    "        inputs = np.zeros((batch_size, 2))\n",
    "        Targets = np.zeros((batch_size, 3))\n",
    "        mini_batch = memory.sample(batch_size)\n",
    "        \n",
    "        for i, (state_b, action_b, reward_b, next_state_b) in enumerate(mini_batch):\n",
    "            inputs[i:i+1] = state_b\n",
    "            target = reward_b\n",
    "            \n",
    "            next_action_for_estimate = decision_net.model.predict(next_state_b).argmax()\n",
    "            q_value_for_estimate = target_net.model.predict(next_state_b)[0][next_action_for_estimate]\n",
    "            target = reward_b + gamma * q_value_for_estimate\n",
    "            \n",
    "            Targets[i][action_b] = target\n",
    "            \n",
    "        self.model.fit(inputs, Targets, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def get_action(self, state, episode, decision_net):   # [C]ｔ＋１での行動を返す\n",
    "        # 徐々に最適行動のみをとる、ε-greedy法\n",
    "        epsilon = 0.001 + 0.9 / (1.0+episode)\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            q_values_for_action = decision_net.model.predict(state)[0]\n",
    "            action = np.argmax(q_values_for_action)  # 最大の報酬を返す行動を選択する\n",
    "\n",
    "        else:\n",
    "            action = np.random.choice([0, 1])  # ランダムに行動する\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.79167402e-01 -2.23429532e-03]\n",
      " [-3.77280371e-01  1.09030105e-03]\n",
      " [-4.15241087e-01  3.05925841e-03]\n",
      " [-4.48024754e-01 -6.99231946e-04]\n",
      " [-4.09522138e-01 -2.52015430e-03]\n",
      " [-3.37958122e-01  1.65150625e-04]\n",
      " [-3.81451355e-01 -2.28395226e-03]\n",
      " [-3.70677484e-01  1.10844018e-04]\n",
      " [-3.88444139e-01 -2.33847550e-03]\n",
      " [-3.75294196e-01  9.58358498e-04]\n",
      " [-4.41297322e-01 -1.75807573e-03]\n",
      " [-4.33743462e-01  1.44951371e-04]\n",
      " [-4.15179901e-01  4.49835656e-03]\n",
      " [-3.66442667e-01 -4.49703695e-03]\n",
      " [-3.71327620e-01  4.24819190e-04]\n",
      " [-3.55116305e-01 -1.55998483e-04]\n",
      " [-4.25759342e-01 -2.36673010e-03]\n",
      " [-4.20766160e-01 -2.86738320e-03]\n",
      " [-4.44636537e-01 -1.97013075e-03]\n",
      " [-3.88091843e-01  2.57874854e-03]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [-1.,  0.,  0.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.],\n",
       "       [ 0.,  0., -1.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 20\n",
    "inputs = np.zeros((batch_size, 2))\n",
    "Targets = np.zeros((batch_size, 3))\n",
    "mini_batch = memory.sample(batch_size)\n",
    "for i, (state_b, action_b, reward_b, next_state_b) in enumerate(mini_batch):\n",
    "    inputs[i:i+1] = state_b\n",
    "    target = reward_b\n",
    "    next_action_for_estimate = decision_net.predict(next_state_b.reshape(1,2)).argmax()\n",
    "    q_value_for_estimate = target_net.predict(next_state_b.reshape(1,2))[0][next_action_for_estimate]\n",
    "    target = reward_b + gamma * q_value_for_estimate\n",
    "    Targets[i][action_b] = target\n",
    "    \n",
    "print(inputs)\n",
    "Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 5],\n",
       "       [1, 2],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.array([1,5]), np.array([1,2]), np.array([1,2])]).reshape(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(max_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.add((observation, action, reward, next_observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.35605739, -0.00057459]), 2, -1.0, array([-0.35683629, -0.0007789 ]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.sample(20)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
